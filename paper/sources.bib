@misc{htmlLivingStandard,
	title        = {HTML Living Standard},
	author       = {{Web Hypertext Application Technology Working Group}},
	year         = 2024,
	month        = {July},
	note         = {accessed 07/30/2024},
	howpublished = {\url{https://html.spec.whatwg.org/multipage/dom.html#current-document-readiness}}
}

@misc{lcpDocumentation,
	title        = {Largest Contentful Paint},
	author       = {{Google}},
	year         = 2020,
	month        = {January},
	note         = {accessed 07/28/2024},
	howpublished = {\url{https://developer.chrome.com/docs/lighthouse/performance/lighthouse-largest-contentful-paint}}
}

@misc{lighthouseVariability,
	title        = {Lighthouse variability},
	author       = {{Google}},
	year         = 2019,
	month        = {May},
	note         = {accessed 08/01/2024},
	howpublished = {\url{https://developers.google.com/web/tools/lighthouse/variability}}
}

@misc{eliminateRenderBlocking,
	title        = {Eliminate render-blocking resources},
	author       = {{Google}},
	year         = 2019,
	month        = {May},
	note         = {accessed 08/01/2024},
	howpublished = {\url{https://developer.chrome.com/docs/lighthouse/performance/render-blocking-resources}}
}

@misc{stateOfJs2023,
	title        = {State of JavaScript 2023},
	author       = {{Devographics}},
	year         = 2024,
	month        = {June},
	note         = {accessed 07/29/2024},
	howpublished = {\url{https://2023.stateofjs.com/en-US/libraries/front-end-frameworks/}}
}

}
@misc{IntersectionObserver,
	title        = {IntersectionObserver},
	author       = {{MDN Mozilla}},
	year         = 2024,
	note         = {accessed 08/06/2024},
	howpublished = {\url{https://developer.mozilla.org/en-US/docs/Web/API/IntersectionObserver}}
}

@misc{renderblocking,
	title        = {Render-blocking},
	author       = {{MDN Mozilla}},
	year         = 2024,
	note         = {accessed 08/09/2024},
	howpublished = {\url{https://developer.mozilla.org/en-US/docs/Glossary/Render_blocking}}
}

@misc{AngularGetStarted,
	title        = {Setting up the local environment and workspace},
	author       = {{Google LLC}},
	year         = 2024,
	note         = {accessed 08/07/2024},
	howpublished = {\url{https://angular.dev/tools/cli/setup-local}}
}

@misc{AstroGetStarted,
	title        = {Install and set up Astro},
	author       = {Schott, Fred K.},
	year         = 2024,
	note         = {accessed 08/07/2024},
	howpublished = {\url{https://docs.astro.build/en/install-and-setup/}}
}

@misc{AstroIslands,
	title        = {Astro Islands},
	author       = {Schott, Fred K.},
	year         = 2024,
	note         = {accessed 09/03/2024},
	howpublished = {\url{https://docs.astro.build/en/concepts/islands/}}
}


@misc{NextGetStarted,
	title        = {Installation},
	author       = {{Vercel, Inc.}},
	year         = 2024,
	note         = {accessed 08/07/2024},
	howpublished = {\url{https://nextjs.org/docs/getting-started/installation}}
}

@misc{NuxtGetStarted,
	title        = {Installation},
	author       = {Chopin, Sébastien and Parsa, Pooya and Roe, Daniel and Fu, Anthony and Lichter, Alexander and Wilton, Harlan and Lucie and Huang, Julien},
	year         = 2024,
	note         = {accessed 08/07/2024},
	howpublished = {\url{https://nuxt.com/docs/getting-started/installation}}
}

@misc{ReactGetStarted,
	title        = {Getting Started},
	author       = {{Meta Platforms, Inc.}},
	year         = 2024,
	note         = {accessed 08/07/2024},
	howpublished = {\url{https://legacy.reactjs.org/docs/getting-started.html}}
}

@misc{SvelteGetStarted,
	title        = {Introduction},
	author       = {{Svelte}},
	year         = 2024,
	note         = {accessed 08/07/2024},
	howpublished = {\url{https://svelte.dev/docs/introduction}}
}

@misc{VueGetStarted,
	title        = {Quick Start},
	author       = {{You, Evan}},
	year         = 2024,
	note         = {accessed 08/07/2024},
	howpublished = {\url{https://vuejs.org/guide/quick-start.html}}
}

@misc{browserUsage,
	title        = {Quick Start},
	author       = {{StatCounter}},
	year         = 2024,
	month        = {July},
	note         = {accessed 07/18/2024},
	howpublished = {\url{https://gs.statcounter.com/}}
}

@misc{navigationTimings,
	title        = {Navigation Timing},
	author       = {{W3C}},
	year         = 2012,
	month        = {December},
	note         = {accessed 07/10/2024},
	howpublished = {\url{https://www.w3.org/TR/navigation-timing/}}
}

@misc{instagram,
	title        = {Instagram},
	author       = {{Instagram from Meta}},
	year         = 2024,
	note         = {accessed 08/02/2024},
	howpublished = {\url{https://www.instagram.com/}}
}

@misc{observedMetrics,
	title        = {Why are the metric values with observed different from those without observed?},
	author       = {Raine, Adam},
	year         = 2024,
	month        = {June},
	note         = {accessed 08/18/2024},
	howpublished = {\url{https://github.com/GoogleChrome/lighthouse/discussions/14190#discussioncomment-3093932}}
}

@misc{pageweightreport,
	title        = {HTTP Archive: Page Weight},
	author       = {Meenan, PÜat and Viscomi, Rick and Calvano, Paul and Pollard, Barry and Ostapenko, Max},
	year         = 2024,
	month        = {Sep},
	note         = {accessed 09/03/2024},
	howpublished = {\url{https://httparchive.org/reports/page-weight}}
}

@inproceedings{webprophet,
	title        = {WebProphet: automating performance prediction for web services},
	author       = {Li, Zhichun and Zhang, Ming and Zhu, Zhaosheng and Chen, Yan and Greenberg, Albert and Wang, Yi-Min},
	year         = 2010,
	booktitle    = {Proceedings of the 7th USENIX Conference on Networked Systems Design and Implementation},
	location     = {San Jose, California},
	publisher    = {USENIX Association},
	address      = {USA},
	series       = {NSDI'10},
	pages        = 10,
	abstract     = {Today, large-scale web services run on complex systems, spanning multiple data centers and content distribution networks, with performance depending on diverse factors in end systems, networks, and infrastructure servers. Web service providers have many options for improving service performance, varying greatly in feasibility, cost and benefit, but have few tools to predict the impact of these options.A key challenge is to precisely capture web object dependencies, as these are essential for predicting performance in an accurate and scalable manner. In this paper, we introduce WebProphet, a system that automates performance prediction for web services. WebProphet employs a novel technique based on timing perturbation to extract web object dependencies, and then uses these dependencies to predict the performance impact of changes to the handling of the objects. We have built, deployed, and evaluated the accuracy and efficiency of WebProphet. Applying WebProphet to the Search and Maps services of Google and Yahoo, we find WebProphet predicts the median and 95th percentiles of the page load time distribution with an error rate smaller than 16\% in most cases. Using Yahoo Maps as an example, we find that WebProphet reduces the problem of performance optimization to a small number of web objects whose optimization would reduce the page load time by nearly 40\%.},
	numpages     = 1
}

@article{webprefetching,
	title        = {Web prefetching performance metrics: A survey},
	author       = {Josep Domènech and José A. Gil and Julio Sahuquillo and Ana Pont},
	year         = 2006,
	journal      = {Performance Evaluation},
	volume       = 63,
	number       = 9,
	pages        = {988--1004},
	doi          = {https://doi.org/10.1016/j.peva.2005.11.001},
	issn         = {0166-5316},
	url          = {https://www.sciencedirect.com/science/article/pii/S0166531605001549},
	keywords     = {Web prefetching, Performance evaluation, Metrics taxonomy},
	abstract     = {Web prefetching techniques have been pointed out to be especially important to reduce perceived web latencies and, consequently, an important amount of work can be found in the open literature. But, in general, it is not possible to do a fair comparison among the proposed prefetching techniques due to three main reasons: (i) the underlying baseline system where prefetching is applied differs widely among the studies; (ii) the workload used in the presented experiments is not the same; (iii) different performance key metrics are used to evaluate their benefits. This paper focuses on the third reason. Our main concern is to identify which are the meaningful indexes when studying the performance of different prefetching techniques. For this purpose, we propose a taxonomy based on three categories, which permits us to identify analogies and differences among the indexes commonly used. In order to check, in a more formal way, the relation between them, we run experiments and estimate statistically the correlation among a representative subset of those metrics. The statistical results help us to suggest which indexes should be selected when performing evaluation studies depending on the different elements in the considered web architecture. The choice of the appropriate key metric is of paramount importance for a correct and representative study. As our experimental results show, depending on the metric used to check the system performance, results cannot only widely vary but also reach opposite conclusions.}
}

@article{surveyofwebmetrics,
	title        = {A survey of Web metrics},
	author       = {Dhyani, Devanshu and Ng, Wee Keong and Bhowmick, Sourav S.},
	year         = 2002,
	month        = {dec},
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 34,
	number       = 4,
	pages        = {469–503},
	doi          = {10.1145/592642.592645},
	issn         = {0360-0300},
	url          = {https://doi.org/10.1145/592642.592645},
	issue_date   = {December 2002},
	abstract     = {The unabated growth and increasing significance of the World Wide Web has resulted in a flurry of research activity to improve its capacity for serving information more effectively. But at the heart of these efforts lie implicit assumptions about "quality" and "usefulness" of Web resources and services. This observation points towards measurements and models that quantify various attributes of web sites. The science of measuring all aspects of information, especially its storage and retrieval or informetrics has interested information scientists for decades before the existence of the Web. Is Web informetrics any different, or is it just an application of classical informetrics to a new medium? In this article, we examine this issue by classifying and discussing a wide ranging set of Web metrics. We present the origins, measurement functions, formulations and comparisons of well-known Web metrics for quantifying Web graph properties, Web page significance, Web page similarity, search and retrieval, usage characterization and information theoretic properties. We also discuss how these metrics can be applied for improving Web information access and use.},
	numpages     = 35,
	keywords     = {Information theoretic, PageRank, Web graph, Web metrics, Web page similarity, quality metrics}
}

@article{webanalyticstools,
	title        = {Web analytics tools and web metrics tools: An overview and comparative analysis},
	author       = {Bekavac, Ivan and Praničević, Daniela Garbin},
	year         = 2015,
	journal      = {Croation Operational Research Review},
	publisher    = {Croation Operational Research Review},
	volume       = 6,
	number       = 2,
	url          = {https://hrcak.srce.hr/en/clanak/218174}
}

@book{usingwebpagetest,
	title        = {Using WebPageTest},
	author       = {Viscomi, Rick and Davies, Andy and Duran, Marcel},
	year         = 2015,
	month        = {Oct},
	publisher    = {O'Reilly Media, Inc.},
	address      = {1005 Gravensetin Highwy North, Sebastopol, CA 95472},
	isbn         = 9781491902592
}

@inproceedings{webperformancepitfalls,
	title        = {Web Performance Pitfalls},
	author       = {Enghardt, Theresa and Zinner, Thomas and Feldmann, Anja},
	year         = 2019,
	booktitle    = {Passive and Active Measurement},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {286--303},
	isbn         = {978-3-030-15986-3},
	editor       = {Choffnes, David and Barcellos, Marinho},
	abstract     = {Web performance is widely studied in terms of load times, numbers of objects, object sizes, and total page sizes. However, for all these metrics, there are various definitions, data sources, and measurement tools. These often lead to different results and almost all studies do not provide sufficient details about the definition of metrics and the data sources they use. This hinders reproducibility as well as comparability of the results. This paper revisits the various definitions and quantifies their impact on performance results. To do so we assess Web metrics across a large variety of Web pages.}
}

@inproceedings{sevenpitfalls,
	title        = {Seven pitfalls to avoid when running controlled experiments on the web},
	author       = {Crook, Thomas and Frasca, Brian and Kohavi, Ron and Longbotham, Roger},
	year         = 2009,
	booktitle    = {Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	location     = {Paris, France},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {KDD '09},
	pages        = {1105–1114},
	doi          = {10.1145/1557019.1557139},
	isbn         = 9781605584959,
	url          = {https://doi.org/10.1145/1557019.1557139},
	abstract     = {Controlled experiments, also called randomized experiments and A/B tests, have had a profound influence on multiple fields, including medicine, agriculture, manufacturing, and advertising. While the theoretical aspects of offline controlled experiments have been well studied and documented, the practical aspects of running them in online settings, such as web sites and services, are still being developed. As the usage of controlled experiments grows in these online settings, it is becoming more important to understand the opportunities and pitfalls one might face when using them in practice. A survey of online controlled experiments and lessons learned were previously documented in Controlled Experiments on the Web: Survey and Practical Guide (Kohavi, et al., 2009). In this follow-on paper, we focus on pitfalls we have seen after running numerous experiments at Microsoft. The pitfalls include a wide range of topics, such as assuming that common statistical formulas used to calculate standard deviation and statistical power can be applied and ignoring robots in analysis (a problem unique to online settings). Online experiments allow for techniques like gradual ramp-up of treatments to avoid the possibility of exposing many customers to a bad (e.g., buggy) Treatment. With that ability, we discovered that it's easy to incorrectly identify the winning Treatment because of Simpson's paradox.},
	numpages     = 10,
	keywords     = {a/b testing, controlled experiments, e-commerce, robot detection, simpson's paradox}
}

@article{whatifanalysis,
	title        = {What-If Analysis of Page Load Time in Web Browsers Using Causal Profiling},
	author       = {Pourghassemi, Behnam and Amiri Sani, Ardalan and Chandramowlishwaran, Aparna},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Meas. Anal. Comput. Syst.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3341617.3326142},
	url          = {https://doi.org/10.1145/3341617.3326142},
	issue_date   = {June 2019},
	abstract     = {Web browsers have become one of the most commonly used applications for desktop and mobile users. Despite recent advances in network speeds and several techniques to speed up web page loading such as speculative loading, smart caching, and multi-threading, browsers still suffer from relatively long page load time (PLT). As web applications are receiving widespread attention owing to their cross-platform support and comparatively straightforward development process, they need to have higher performance to compete with native applications. Recent studies have investigated the bottleneck of the modern web browser's performance and conclude that network connection is not the browser's bottleneck anymore. Even though there is still no consensus on this claim, no subsequent analysis has been conducted to inspect which parts of the browser's computation contribute to the performance overhead. In this paper, we apply comprehensive and quantitative what-if analysis on the web browser's page loading process. Unlike conventional profiling methods, we applycausal profiling to precisely determine the impact of each computation stage such as HTML parsing and Layout on PLT. For this purpose, we develop COZ+, a high-performance causal profiler capable of analyzing large software systems such as the Chromium browser. COZ+ highlights the most influential spots for further optimization, which can be leveraged by browser developers and/or website designers. For instance, COZ+ shows that optimizing JavaScript by 40\% is expected to improve the Chromium desktop browser's page loading performance by more than 8.5\% under typical network conditions.},
	articleno    = 27,
	numpages     = 23,
	keywords     = {what-if analysis, web browsers, page load time, causal profiling}
}

@article{analyzingfactors,
	title        = {Analyzing factors that influence end-to-end Web performance},
	author       = {Balachander Krishnamurthy and Craig E Wills},
	year         = 2000,
	journal      = {Computer Networks},
	volume       = 33,
	number       = 1,
	pages        = {17--32},
	doi          = {https://doi.org/10.1016/S1389-1286(00)00067-0},
	issn         = {1389-1286},
	url          = {https://www.sciencedirect.com/science/article/pii/S1389128600000670},
	keywords     = {Web performance, Web protocols, End-to-end performance, Active measurement},
	abstract     = {Web performance impacts the popularity of a particular Web site or service as well as the load on the network, but there have been no publicly available end-to-end measurements that have focused on a large number of popular Web servers examining the components of delay or the effectiveness of the recent changes to the HTTP protocol. In this paper we report on an extensive study carried out from many client sites geographically distributed around the world to a collection of over 700 servers to which a majority of Web traffic is directed. Our results show that the HTTP/1.1 protocol, particularly with pipelining, is indeed an improvement over existing practice, but that servers serving a small number of objects or closing a persistent connection without explicit notification can reduce or eliminate any performance improvement. Similarly, use of caching and multi-server content distribution can also improve performance if done effectively.}
}

@book{highperformancebrowsernetworking,
	title        = {High Performance Browser Networking},
	author       = {Grigorik, Ilya},
	year         = 2013,
	month        = {Sep},
	publisher    = {O'Reilly Media, Inc.},
	address      = {1005 Gravensetin Highwy North, Sebastopol, CA 95472},
	isbn         = 9781449344764
}

@inproceedings{measuringandmitigatinwebperformance,
	title        = {Community contribution award -- Measuring and mitigating web performance bottlenecks in broadband access networks},
	author       = {Sundaresan, Srikanth and Feamster, Nick and Teixeira, Renata and Magharei, Nazanin},
	year         = 2013,
	booktitle    = {Proceedings of the 2013 Conference on Internet Measurement Conference},
	location     = {Barcelona, Spain},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {IMC '13},
	pages        = {213–226},
	doi          = {10.1145/2504730.2504741},
	isbn         = 9781450319539,
	url          = {https://doi.org/10.1145/2504730.2504741},
	abstract     = {We measure Web performance bottlenecks in home broadband access networks and evaluate ways to mitigate these bottlenecks with caching within home networks. We first measure Web performance bottlenecks to nine popular Web sites from more than 5,000 broadband access networks and demonstrate that when the downstream throughput of the access link exceeds about 16 Mbits/s, latency is the main bottleneck for Web page load time. Next, we use a router-based Web measurement tool, Mirage, to deconstruct Web page load time into its constituent components (DNS lookup, TCP connection setup, object download) and show that simple latency optimizations can yield significant improvements in overall page load times. We then present a case for placing a cache in the home network and deploy three common optimizations: DNS caching, TCP connection caching, and content caching. We show that caching only DNS and TCP connections yields significant improvements in page load time, even when the user's browser is already performing similar independent optimizations. Finally, we use traces from real homes to demonstrate how prefetching DNS and TCP connections for popular sites in a home-router cache can achieve faster page load times.},
	numpages     = 14,
	keywords     = {web performance, popularity-based prefetching, connection caching, broadband networks, bottlenecks, DNS prefetching}
}

@inproceedings{onlandingandinternalwebpages,
	title        = {On Landing and Internal Web Pages: The Strange Case of Jekyll and Hyde in Web Performance Measurement},
	author       = {Aqeel, Waqar and Chandrasekaran, Balakrishnan and Feldmann, Anja and Maggs, Bruce M.},
	year         = 2020,
	booktitle    = {Proceedings of the ACM Internet Measurement Conference},
	location     = {Virtual Event, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {IMC '20},
	pages        = {680–695},
	doi          = {10.1145/3419394.3423626},
	isbn         = 9781450381383,
	url          = {https://doi.org/10.1145/3419394.3423626},
	abstract     = {There is a rich body of literature on measuring and optimizing nearly every aspect of the web, including characterizing the structure and content of web pages, devising new techniques to load pages quickly, and evaluating such techniques. Virtually all of this prior work used a single page, namely the landing page (i.e., root document, "/"), of each web site as the representative of all pages on that site. In this paper, we characterize the differences between landing and internal (i.e., non-root) pages of 1000 web sites to demonstrate that the structure and content of internal pages differ substantially from those of landing pages, as well as from one another. We review more than a hundred studies published at top-tier networking conferences between 2015 and 2019, and highlight how, in light of these differences, the insights and claims of nearly two-thirds of the relevant studies would need to be revised for them to apply to internal pages.Going forward, we urge the networking community to include internal pages for measuring and optimizing the web. This recommendation, however, poses a non-trivial challenge: How do we select a set of representative internal web pages from a web site? To address the challenge, we have developed Hispar, a "top list" of 100,000 pages updated weekly comprising both the landing pages and internal pages of around 2000 web sites. We make Hispar and the tools to recreate or customize it publicly available.},
	numpages     = 16,
	keywords     = {PLT, QoE, Speed Index, Web page performance, top lists}
}

@book{integratedapproach,
	title        = {Integrated Approach to Web Performance Testing: A Practitioner's Guide},
	author       = {Subraya, B.M},
	year         = 2006,
	publisher    = {Idea Group Inc.},
	address      = {701 E Chocolate Avenua, Suite 200, Hershey PA 17033-1240},
	isbn         = 1591407850
}

@article{performancecomparison,
	title        = {Performance comparison of dynamic web platforms},
	author       = {Varsha Apte and Tony Hansen and Paul Reeser},
	year         = 2003,
	journal      = {Computer Communications},
	volume       = 26,
	number       = 8,
	pages        = {888--898},
	doi          = {https://doi.org/10.1016/S0140-3664(02)00221-9},
	issn         = {0140-3664},
	url          = {https://www.sciencedirect.com/science/article/pii/S0140366402002219},
	note         = {Performance evaluation of IP networks and services},
	keywords     = {Dynamic, Web, Common Gateway Interface, FastCGI, C++, Java, Servlets, Java Server Pages, Performance, Comparison},
	abstract     = {Over the last few years, the World Wide Web has transformed itself from a static content-distribution medium to an interactive, dynamic medium. The Web is now widely used as the presentation layer for a host of on-line services such as e-mail and address books, e-cards, e-calendar, shopping, banking, and stock trading. As a consequence (HyperText Markup Language)HTML files are now typically generated dynamically after the server receives the request. From the Web-site providers' point of view, dynamic generation of HTML pages implies a lesser understanding of the real capacity and performance of their Web servers. From the Web developers' point of view, dynamic content implies an additional technology decision: the Web programming technology to be employed in creating a Web-based service. Since the Web is inherently interactive, performance is a key requirement, and often demands careful analysis of the systems. In this paper, we compare four dynamic Web programming technologies from the point of view of performance. The comparison is based on testing and measurement of two cases: one is a case study of a real application that was deployed in an actual Web-based service; the other is a trivial application. The two cases provide us with an opportunity to compare the performance of these technologies at two ends of the spectrum in terms of complexity. Our focus in this paper is on how complex vs. simple applications perform when implemented using different Web programming technologies. The paper draws comparisons and insights based on this development and performance measurement effort.}
}

@inproceedings{viewcentricperformanceoptimization,
	title        = {View-Centric Performance Optimization for Database-Backed Web Applications},
	author       = {Yang, Junwen and Yan, Cong and Wan, Chengcheng and Lu, Shan and Cheung, Alvin},
	year         = 2019,
	month        = {May},
	booktitle    = {2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},
	volume       = {},
	number       = {},
	pages        = {994--1004},
	doi          = {10.1109/ICSE.2019.00104},
	issn         = {1558-1225},
	abstract     = {Web developers face the stringent task of designing informative web pages while keeping the page-load time low. This task has become increasingly challenging as most web contents are now generated by processing ever-growing amount of user data stored in back-end databases. It is difficult for developers to understand the cost of generating every web-page element, not to mention explore and pick the web design with the best trade-off between performance and functionality. In this paper, we present Panorama, a view-centric and database-aware development environment for web developers. Using database-aware program analysis and novel IDE design, Panorama provides developers with intuitive information about the cost and the performance-enhancing opportunities behind every HTML element, as well as suggesting various global code refactorings that enable developers to easily explore a wide spectrum of performance and functionality trade-offs.},
	keywords     = {Databases;Rails;Web pages;Data processing;Tools;Servers;Task analysis;database backed web applications;ORM framework;view centric}
}

@article{dynamicthreadassignment,
	title        = {Dynamic thread assignment in web server performance optimization},
	author       = {Wemke {van der Weij} and Sandjai Bhulai and Rob {van der Mei}},
	year         = 2009,
	journal      = {Performance Evaluation},
	volume       = 66,
	number       = 6,
	pages        = {301--310},
	doi          = {https://doi.org/10.1016/j.peva.2008.11.001},
	issn         = {0166-5316},
	url          = {https://www.sciencedirect.com/science/article/pii/S0166531608001089},
	keywords     = {Dynamic programming, Dynamic thread management, Multi-layered queueing systems, Web servers},
	abstract     = {Popular web sites are expected to handle huge number of requests concurrently within a reasonable time frame. The performance of these web sites is largely dependent on effective thread management of their web servers. Although the implementation of static and dynamic thread policies is common practice, remarkably little is known about the implications on performance. Moreover, the commonly used policies do not take into account the complex interaction between the threads that compete for access to a shared resource. We propose new dynamic thread-assignment policies that minimize the average response time of web servers. The web server is modeled as a two-layered tandem of multi-threading queues, where the active threads compete for access to a common resource. This type of two-layered queueing model, which occurs naturally in the performance modeling of systems with intensive software–hardware interaction, are on the one hand appealing from an application point of view, but on the other hand are challenging from a methodological point of view. Our results show that the optimal dynamic thread-assignment policies yield strong reductions in the response times. Validation on an Apache web server shows that our dynamic thread policies confirm our analytical results.}
}

@article{measuringweblatency,
	title        = {Measuring Web Latency and Rendering Performance: Method, Tools, and Longitudinal Dataset},
	author       = {Asrese, Alemnew Sheferaw and Eravuchira, Steffie Jacob and Bajpai, Vaibhav and Sarolahti, Pasi and Ott, Jörg},
	year         = 2019,
	month        = {June},
	journal      = {IEEE Transactions on Network and Service Management},
	volume       = 16,
	number       = 2,
	pages        = {535--549},
	doi          = {10.1109/TNSM.2019.2896710},
	issn         = {1932-4537},
	abstract     = {This paper presents Webget, a measurement tool that measures Web quality of service (QoS) metrics, including the DNS lookup time, time to first byte (TTFB), and the download time. Webget also captures Web complexity metrics, such as the number and the size of objects that make up the website. We deploy the Webget test to measure the Web performance of Google, YouTube, and Facebook from 182 SamKnows probes. Using a 3.5-year-long (January 2014-July 2017) dataset, we show that the DNS lookup time of these popular content delivery networks (CDNs) and the download time of Google have improved over time. We also show that the TTFB toward Facebook exhibits worse performance than the Google CDN. Moreover, we show that the number and the size of objects are not the only factors that affect the Web download time. We observe that these webpages perform differently across regions and service providers. We also developed a Web measurement system, Web performance and rendering (WePR) that measures the same Web QoS and complexity metrics as Webget, but it also captures the Web quality of experience metrics, such as rendering time. WePR has a distributed architecture where the component that measures the Web QoS and complexity metrics is deployed on the SamKnows probe, while the rendering time is calculated on a central server. We measured the rendering performance of four websites. We show that in 80% of the cases, the rendering time of the websites is faster than the downloading time. The source code of the WePR system and the dataset is made publicly available.},
	keywords     = {Rendering (computer graphics);Probes;Google;Quality of service;Measurement;Complexity theory;Facebook;Web latency;Web performance;Web rendering;Web sites;Webget;WePR;WebPerf;Web QoE}
}

@article{effectsofwebpagecontents,
	title        = {Effects of Web Page Contents on Load Time over the Internet},
	author       = {Zhou, Munyaradzi and Giyane, Maxmillan and Nyasha, Mutembedza},
	year         = 2013,
	month        = 10,
	journal      = {International Journal of Science and Research (IJSR)},
	pages        = {2319--7064}
}

@article{loadtestingofwebsites,
	title        = {Load testing of Web sites},
	author       = {Menasce, D.A.},
	year         = 2002,
	month        = {July},
	journal      = {IEEE Internet Computing},
	volume       = 6,
	number       = 4,
	pages        = {70--74},
	doi          = {10.1109/MIC.2002.1020328},
	issn         = {1941-0131},
	abstract     = {Developers typically measure a Web application's quality of service in terms of response time, throughput, and availability. Poor QoS translates into frustrated customers, which can lead to lost business opportunities. At the same time, company expenditures on a Web site's IT infrastructure are a function of the site's expected traffic. Ideally, you want to spend enough, and no more, allocating resources where they will generate the most benefit. For example, you should not upgrade your Web servers if customers experience most delays in the database server or load balancer. Thus, to maximize your ROI, you must determine when and how to upgrade IT infrastructure. One way to assess IT infrastructure performance is through load testing, which lets you assess how your Web site supports its expected workload by running a specified set of scripts that emulate customer behavior at different load levels. I describe the QoS factors load testing addresses, how to conduct load testing, and how it addresses business needs at several requirement levels.},
	keywords     = {Testing;Quality of service;Delay;Time measurement;Throughput;Availability;Companies;Resource management;Web server;Databases}
}

@inproceedings{typescript,
	title        = {Understanding TypeScript},
	author       = {Bierman, Gavin and Abadi, Martin and Torgersen, Mads},
	year         = 2014,
	booktitle    = {ECOOP 2014 -- Object-Oriented Programming},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {257--281},
	isbn         = {978-3-662-44202-9},
	editor       = {Jones, Richard},
	abstract     = {TypeScript is an extension of JavaScript intended to enable easier development of large-scale JavaScript applications. While every JavaScript program is a TypeScript program, TypeScript offers a module system, classes, interfaces, and a rich gradual type system. The intention is that TypeScript provides a smooth transition for JavaScript programmers---well-established JavaScript programming idioms are supported without any major rewriting or annotations. One interesting consequence is that the TypeScript type system is not statically sound by design. The goal of this paper is to capture the essence of TypeScript by giving a precise definition of this type system on a core set of constructs of the language. Our main contribution, beyond the familiar advantages of a robust, mathematical formalization, is a refactoring into a safe inner fragment and an additional layer of unsafe rules.}
}

@article{internetconnections,
	title        = {Relative fixed Internet connection speed experiences as antecedents of customer satisfaction and loyalty: An empirical analysis of consumers in Germany},
	author       = {Gerpott, Torsten J.},
	year         = 2018,
	journal      = {Management \& Marketing},
	volume       = 13,
	number       = 4,
	pages        = {1150--1173},
	doi          = {doi:10.2478/mmcks-2018-0029},
	url          = {https://doi.org/10.2478/mmcks-2018-0029}
}
